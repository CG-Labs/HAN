import spektral
from spektral.layers import GCNConv
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dropout, Dense
import numpy as np
import pickle

class GNNModel(Model):
    def __init__(self, num_classes, num_features, **kwargs):
        """
        Initialize the Graph Neural Network model with the given number of classes and features.

        Parameters:
        - num_classes: int, the number of classes for classification tasks.
        - num_features: int, the number of features in the input data.
        """
        super(GNNModel, self).__init__(**kwargs)
        self.conv1 = GCNConv(16, activation='relu')
        self.conv2 = GCNConv(num_classes, activation='softmax')
        self.dropout = Dropout(0.5)
        self.num_features = num_features
        # Adding a dense layer for regression prediction
        self.dense = Dense(1, activation='linear')

    def call(self, inputs, training=False):
        """
        Forward pass for the model.

        Parameters:
        - inputs: tuple, containing the feature matrix and adjacency matrix.
        - training: bool, indicating whether the call is for training or inference.

        Returns:
        - The output of the last layer of the model.
        """
        x, adjacency = inputs
        x = self.conv1([x, adjacency])
        x = self.dropout(x, training=training)
        x = self.conv2([x, adjacency])
        # Using the dense layer to output a single value for regression
        x = self.dense(x)
        return x

    def analyze_data(self, processed_data):
        """
        Analyze the processed data using the GNN model to generate embeddings.

        Parameters:
        - processed_data: tuple, containing the feature matrix and adjacency matrix.

        Returns:
        - A dictionary with the embeddings generated by the model.
        """
        x, adjacency = processed_data
        embeddings = self.call((x, adjacency), training=False)
        return {'embeddings': embeddings.numpy()}

    def make_prediction(self, analysis_results):
        """
        Make predictions based on the analysis results using the GNN model.

        Parameters:
        - analysis_results: dict, containing the embeddings from the analyze_data method.

        Returns:
        - A dictionary with the prediction value.
        """
        embeddings = analysis_results['embeddings']
        # Using the dense layer to make a prediction from embeddings
        prediction = self.dense(embeddings)
        return {'prediction': prediction.numpy()[0]}

    def train_model(self, feature_matrix, adjacency_matrix, labels, epochs=200, learning_rate=0.01):
        """
        Train the GNN model with the provided feature matrix and adjacency matrix.

        Parameters:
        - feature_matrix: ndarray, the feature matrix for the GNN.
        - adjacency_matrix: ndarray, the adjacency matrix for the GNN.
        - labels: ndarray, the labels for the training data.
        - epochs: int, the number of epochs to train the model.
        - learning_rate: float, the learning rate for the optimizer.

        Returns:
        - A dictionary indicating the status of the training process.
        """
        optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)
        loss_fn = tf.keras.losses.MeanSquaredError()
        for epoch in range(epochs):
            with tf.GradientTape() as tape:
                predictions = self.call((feature_matrix, adjacency_matrix), training=True)
                loss = loss_fn(labels, predictions)
            gradients = tape.gradient(loss, self.trainable_variables)
            optimizer.apply_gradients(zip(gradients, self.trainable_variables))
            print(f"Epoch {epoch}: Loss: {loss.numpy()}")
        return {'status': 'Model trained successfully'}

# Load the feature matrix and adjacency matrix
with open('feature_matrix.pkl', 'rb') as f:
    feature_matrix = pickle.load(f)

with open('adjacency_matrix.pkl', 'rb') as f:
    adjacency_matrix = pickle.load(f)

# Load the labels for the training data
with open('labels.pkl', 'rb') as f:
    labels = pickle.load(f)

# Initialize the GNN model
num_classes = 1  # For regression, we have one output
num_features = feature_matrix.shape[1]
gnn_model = GNNModel(num_classes=num_classes, num_features=num_features)

# Train the model
gnn_model.train_model(feature_matrix, adjacency_matrix, labels)
